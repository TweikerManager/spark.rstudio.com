---
title: "Configuring and deploying sparklyr"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

## Installation

### Versions

You can use the `spark_install` function to install various versions of Spark on your system. For example:

```{r}
spark_install(version = "1.6.1", hadoop_version = "2.6")
```

This will install specified version of Spark within a shared directory for all Spark installations.

The following functions are available for managing Spark installations:

| Function | Description  |
|----------------------------|---------------------------------------------|
| [`spark_install`](reference/sparklyr/latest/spark_install.html) | Install a version of Spark by spark/hadoop version.|
| [`spark_install_tar`](reference/sparklyr/latest/spark_install.html) | Install a version of Spark from a Spark distribution tarball. |
| [`spark_available_versions`](reference/sparklyr/latest/spark_install.html) | List all versions of Spark that can be installed via `spark_install`. |
| [`spark_installed_versions`](reference/sparklyr/latest/spark_install.html) | List all versions of Spark currently installed. |
| [`spark_install_dir`](reference/sparklyr/latest/spark_install.html) | Determine the directory where versions of Spark are installed. |


### Install Directory

The location of the Spark installation directory can be determined by calling the `spark_install_dir` function. By default this directory is a platform-specific location for application support files (determined using the [rappdirs](https://github.com/hadley/rappdirs) package). However, you can change the location by setting the `spark.install.dir` option. For example:

```{r}
options(spark.install.dir = "/opt/spark")
```

In server environments (e.g. when using RStudio Server or Shiny Server) it's highly recommended that system administrators install whichever Spark versions are required within a shared directory (e.g. `/opt/spark`) and then set the `spark.install.dir` option system-wide via [Rprofile.site](https://stat.ethz.ch/R-manual/R-devel/library/base/html/Startup.html).


## Deployment

There are two well supported deployment modes for sparklyr: developing locally (typically on the desktop with smaller/sampled datasets) and working directly on the Spark cluster via RStudio Server.

### Local

You can easily download and install a local version of Spark on your desktop using the `spark_install` function described above. This is useful both for learning and experimentation, but also as a development environment for analyses, reports, and applications that you plan to eventually deploy to a multi-node Spark cluster.

For the local development scenario, see the [Configuration] section below for details on how to have the same code work seamlessly in both development and production environments.

### Cluster Node

For production use the recommended configuration is to run [RStudio Server](https://www.rstudio.com/products/rstudio/) directly on one of the cluster nodes. 

## Configuration 

This section describes the various options available for configuring both the behavior of the **sparklyr** package as well as the underlying Spark cluster. Creating multiple configuration profiles (e.g. development, test, production) is also covered.

### Config Files

By default the sparklyr package reads it's configuration from a file named `config.yml` located in the current working directory (this file is not required and only need be provided for overriding default behavior). The `config.yml` file is in turn processed using the [config](https://github.com/rstudio/config) package, which enables support for multiple named configuration profiles.

### Package Options

There are a number of options available to configure the behavior of the sparklyr package:

| Option | Description  |
|----------------------------|---------------------------------------------|
| `sparklyr.defaultPackages`| Spark packages to automatically include within session (defaults to "com.databricks:spark-csv_2.11:1.3.0" and "com.amazonaws:aws-java-sdk-pom:1.10.34") |
| `sparklyr.cores.local` | Number of cores to use when running in local mode (defaults to `parallel::detectCores`) |
| `sparklyr.shell.*` | Command line parameters to pass to `spark-shell` (see the [Spark documentation](https://spark.apache.org/docs/latest/submitting-applications.html) for details on supported options) |

For example, this configuration file sets the number of local cores to 4 and the amount of memory allocated for the Spark driver to 2G:

```yaml
default:
  sparklyr.cores.local: 4
  sparklyr.shell.driver-memory: 4GB
```

Note that the use of `default` will be explained below in [Multiple Profiles].

### Spark Options

You can also use `config.yml` to specify arbitrary Spark configuration properties:

| Option | Description  |
|----------------------------|---------------------------------------------|

| `spark.*` Arbitrary configuration properties (applied by creating a `SparkConf` containing the specified properties). See the [Spark Configuration](http://spark.apache.org/docs/latest/configuration.html) for documentation on available properties.
| `spark.sql.*`| Arbitrary configuration properties for Spark SQL (applied using SET). See the Spark [SQL Programming Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html) for documentation on available properties. |

For example, this configuration file set a custom scratch directory for Spark and specifies 100 as the number of partitions to use when shuffling data for joins or aggregations:

```yaml
default:
  spark.local.dir: /tmp/spark-scratch
  spark.sql.shuffle.partitions: 100
```

### User Options

You can also include arbitrary custom user options within the `config.yml` file. These can be named anything you like so long as they *do not* use either `spark` or `sparklyr` as a prefix. For example, this configuration file defines `dataset` and `sample-size` options:

```yaml
default:
  dataset: "observations.parquet"
  sample-size: 10000
```

### Multiple Profiles

The [config](https://github.com/rstudio/config) package enables the definition of multiple named configuration profiles for different environments (e.g. default, test, production). All environments automatically inherit from the `default` environment and can optionally also inherit from each other.

For example, you might want to use a distinct datasets for development and testing or might want to use custom Spark configuration properties that are only applied when running on a production cluster. Here's how that would expressed in `config.yml`:

```yaml
default:
  dataset: "observations-dev.parquet"
  sample-size: 10000
  
production:
  spark.memory.fraction: 0.9
  spark.rdd.compress: true
  dataset: "observations.parquet"
  sample-size: null
```

The currently active configuration is determined via the value of `R_CONFIG_ACTIVE` environment variable. See the [config package documentation](https://github.com/rstudio/config) for additional details.

## EC2

Spark provides some utilities that make it simpler to provision a Spark cluster on Amazon EC2. sparklyr wraps these utilities to make them available from within R. You should realize, however, that the default security settings for such clusters are very permissive; you should not use this technique to create a cluster that hosts any sensitive data. 

### Deploying a Cluster

Deploy a 1-master 1-worker cluster using:

```{r, eval=FALSE}
cl <- spark_ec2_cluster(access_key_id = "AAAAAAAAAAAAAAAAAAAA",
                        secret_access_key = "1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1",
                        pem_file = "spark.pem")
```

This gives you a handle to an unprovisioned cluster on EC2. You can bring this cluster to life by running `spark_ec2_deploy(cl)` or later destroy the cluster using `spark_ec2_destroy(cl)`.

To bring this cluster to life:

```{r, eval=FALSE}
spark_ec2_deploy(cl)
```

You may need to remotely SSH into the master node to manage the system or install system-level dependencies. The following R command will give you the command you need to run in a terminal shell to log in to the Spark master on EC2:

```{r, eval=FALSE}
spark_ec2_login(cl)
```

Once connected in a terminal shell, change the password for the `rstudio` user using:

`passwd rstudio`

You now have a Spark cluster running remotely on EC2. You can interact with this cluster in one of two ways: 

1. You can login to the cluster remotely using SSH and an instance of RStudio already installed on the cluster; or 

2. You can connect to this remote cluster from your local machine, using your own computer to run R and only using this EC2 cluster to host Spark for you.

### RStudio Server

You can choose to login to the cluster and run your R code on the cluster itself. The EC2 cluster has an instance of [RStudio Server](https://www.rstudio.com/products/rstudio/#Server) installed on it by default. Run the following command to open that instance in your web browser.

```{r, eval=FALSE}
spark_ec2_rstudio(cl)
```

#### Install Dependencies

Using the remote SSH terminal into the cluster, run:

```
yum -y install openssl-devel libcurl libcurl-devel
```

Now in the RStudio window that you opened by running `spark_ec2_rstudio()`, run the following R commands to install the packages you'll want on this cluster.

```{r, eval=FALSE}
# install the latest development version of devtools (required)
install.packages("devtools")
devtools::install_github("hadley/devtools")
devtools::reload(devtools::inst("devtools"))

# install sparklyr
devtools::install_github("rstudio/sparklyr", auth_token = "56aef3d82d3ef05755e40a4f6bdaab6fbed8a1f1")
```


#### Connect to Spark

Once you've installed all the necessary packages on the RStudio instance running on the EC2 Spark cluster, you can establish a connection to Spark by running:

```{r, eval=FALSE}
master <- system('cat /root/spark-ec2/cluster-url', intern=TRUE)
sc <- spark_connect(master)

src_tbls(sc)
```

### Remote Connection

The alternative approach to leverage the cluster on EC2 would be to run sparklyr on your own machine locally, and connect from your local machine to the remote Spark instance.

This connection isn't permitted in the default configuration of the cluster, so you'll need to [open port 7077 in the EC2 security group](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html) associated with your Spark Master. You can enable TCP traffic on port 7077 from whatever IP addresses will be running sparklyr that you want to be able to connect.

Once you've changed the firewall settings, you can now connect to your EC2 Spark cluster from a local sparklyr. In the IDE, you can merely click the "New Connection" button, then configure "Master" to be a "Remote server..." and provide the address and port of your remote EC2 instance. You can find this information by running `spark_ec2_web(cl)` and pulling the value for the `URL` field on that page. It should look something like `spark://1.2.3.4.compute-1.amazonaws.com:7077`. Use that value for the "Remote Server" URL in your local IDE.





