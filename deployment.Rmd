---
title: "Configuring and deploying sparklyr"
---

## Introduction

- Use case / justification
- Local vs. remote execution (diagram?)

## Spark Installation

- Managing and using multiple versions
- Local development mode
- Server deployment mode

## Configuration (config package)

- Package options
- Spark options (http://spark.apache.org/docs/latest/configuration.html)
- User options
- Multiple configurations
    - Use case: smaller datasets for development e.g. local vs. staging vs. production

## EC2

Spark provides some utilities that make it simpler to provision a Spark cluster on Amazon EC2. sparklyr wraps these utilities to make them available from within R. You should realize, however, that the default security settings for such clusters are very permissive; you should not use this technique to create a cluster that hosts any sensitive data. 

### Deploying a Cluster

Deploy a 1-master 1-worker cluster using:

```{r, eval=FALSE}
cl <- spark_ec2_cluster(access_key_id = "AAAAAAAAAAAAAAAAAAAA",
                        secret_access_key = "1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1",
                        pem_file = "spark.pem")
```

This gives you a handle to an unprovisioned cluster on EC2. You can bring this cluster to life by running `spark_ec2_deploy(cl)` or later destroy the cluster using `spark_ec2_destroy(cl)`.

To bring this cluster to life:

```{r, eval=FALSE}
spark_ec2_deploy(cl)
```

You may need to remotely SSH into the master node to manage the system or install system-level dependencies. The following R command will give you the command you need to run in a terminal shell to log in to the Spark master on EC2:

```{r, eval=FALSE}
spark_ec2_login(cl)
```

Once connected in a terminal shell, change the password for the `rstudio` user using:

`passwd rstudio`

You now have a Spark cluster running remotely on EC2. You can interact with this cluster in one of two ways: 

1. You can login to the cluster remotely using SSH and an instance of RStudio already installed on the cluster; or 

2. You can connect to this remote cluster from your local machine, using your own computer to run R and only using this EC2 cluster to host Spark for you.

### RStudio Server

You can choose to login to the cluster and run your R code on the cluster itself. The EC2 cluster has an instance of [RStudio Server](https://www.rstudio.com/products/rstudio/#Server) installed on it by default. Run the following command to open that instance in your web browser.

```{r, eval=FALSE}
spark_ec2_rstudio(cl)
```

#### Install Dependencies

Using the remote SSH terminal into the cluster, run:

```
yum -y install openssl-devel libcurl libcurl-devel
```

Now in the RStudio window that you opened by running `spark_ec2_rstudio()`, run the following R commands to install the packages you'll want on this cluster.

```{r, eval=FALSE}
# install the latest development version of devtools (required)
install.packages("devtools")
devtools::install_github("hadley/devtools")
devtools::reload(devtools::inst("devtools"))

# install sparklyr
devtools::install_github("rstudio/sparklyr", auth_token = "56aef3d82d3ef05755e40a4f6bdaab6fbed8a1f1")
```


#### Connect to Spark

Once you've installed all the necessary packages on the RStudio instance running on the EC2 Spark cluster, you can establish a connection to Spark by running:

```{r, eval=FALSE}
master <- system('cat /root/spark-ec2/cluster-url', intern=TRUE)
sc <- spark_connect(master)

src_tbls(sc)
```

### Remote Connection

The alternative approach to leverage the cluster on EC2 would be to run sparklyr on your own machine locally, and connect from your local machine to the remote Spark instance.

This connection isn't permitted in the default configuration of the cluster, so you'll need to [open port 7077 in the EC2 security group](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html) associated with your Spark Master. You can enable TCP traffic on port 7077 from whatever IP addresses will be running sparklyr that you want to be able to connect.

Once you've changed the firewall settings, you can now connect to your EC2 Spark cluster from a local sparklyr. In the IDE, you can merely click the "New Connection" button, then configure "Master" to be a "Remote server..." and provide the address and port of your remote EC2 instance. You can find this information by running `spark_ec2_web(cl)` and pulling the value for the `URL` field on that page. It should look something like `spark://1.2.3.4.compute-1.amazonaws.com:7077`. Use that value for the "Remote Server" URL in your local IDE.





