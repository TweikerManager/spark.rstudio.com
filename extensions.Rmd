---
title: "Creating Extensions for sparklyr"
output:
  html_document:
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

```{r connect, eval=FALSE, include=FALSE}
library(sparklyr)
sc <- spark_connect(master = "local", version = "1.6.1")
```


## Introduction

The sparklyr package provides a [dplyr](dplyr.html) interface to Spark DataFrames as well as an R interface to [MLlib](mllib.html). However, since Spark is a general-purpose cluster computing system there are many other R interfaces that could be built (e.g. interfaces to custom machine learning pipelines, interfaces to 3rd party Spark packages, etc.).

The facilities used internally by sparklyr for its dplyr and MLlib interfaces are available to extension packages. This guide describes how you can use these tools to create your own custom R interfaces to Spark.

To [sparkapi](https://github.com/rstudio/sparkapi) package provides an API which can be used to write extensions for the sparklyr package. Our hope is that this API will evolve into a common extension API shared with [SparkR](https://spark.apache.org/docs/latest/sparkr.html), enabling extension packages to work seamlessly with both sparklyr and SparkR.

Please also note that these APIs are preliminary and subject to review and modification as we work towards making them available in both sparklyr and SparkR. 

## Calling Spark from R

There are several functions available for calling the methods of Java objects and static methods of Java classes:

| Function | Description  |
|----------------------------|---------------------------------------------|
| [`sparkapi_invoke`](reference/sparkapi/latest/sparkapi_invoke.html) | Call a method on an object. |
| [`sparkapi_invoke_new`](reference/sparkapi/latest/sparkapi_invoke.html) | Create a new object by invoking a constructor.  |
| [`sparkapi_invoke_static`](reference/sparkapi/latest/sparkapi_invoke.html) | Call a static method on an object.  |

For example, to create a new instance of the `java.math.BigInteger` class and then call the `longValue()` method on it you would use code like this:

```{r}
library(sparkapi)
billionBigInteger <- sparkapi_invoke_new(sc, "java.math.BigInteger", "1000000000")
billion <- sparkapi_invoke(billionBigInteger, "longValue")
```

This code can be re-written to be more compact and clear using [magrittr](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html) pipes: 

```{r}
billion <- sc %>% 
  sparkapi_invoke_new("java.math.BigInteger", "1000000000") %>%
    sparkapi_invoke("longValue")
```

This syntax is similar to the method-chaining syntax often used with Scala code, so we'll use this form for the remainder of this guide.

Calling a static method of a class is also straightforward. For example, to call the `Math::hypot()` static function you would use this code:

```{r}
hypot <- sc %>% sparkapi_invoke_static("java.lang.Math", "hypot", 10, 20) 
```

Note that arguments to methods are included immediately after the name of the method to be called.

## Wrapper Functions

Creating an extension typically consists of writing R wrapper functions for a set of Spark services. In this section we'll describe the typical form of these functions as well as how to handle special types like Spark DataFrames. 

Here's an example of a wrapper function for the the text file line counting function available from the SparkContext object:

```{r}
count_lines <- function(sc, path) {
  sparkapi_spark_context(sc) %>%
    sparkapi_invoke("textFile", path, 1L) %>%
    sparkapi_invoke("count")
}
```

The `count_lines` function takes a `sparkapi_connection` (`sc`) argument which enables it to obtain a reference to the `SparkContext` object. Creating new objects also requires the `sc` argument.

In some cases you'll write wrapper functions that accept references to Spark objects (for example, a Spark DataFrame). In this scenario the following functions are also useful:

| Function | Description  |
|----------------------------|---------------------------------------------|
| [`sparkapi_connection`](reference/sparkapi/latest/sparkapi_connection.html) | Get the Spark connection associated with objects of various types. |
| [`sparkapi_spark_context`](reference/sparkapi/latest/sparkapi_spark_context.html) | Get the SparkContext for a `sparkapi_connection` |
| [`sparkapi_hive_context`](reference/sparkapi/latest/sparkapi_hive_context.html) | Get the HiveContext for a `sparkapi_connection` |
| [`sparkapi_jobj`](reference/sparkapi/latest/sparkapi_jobj.html) | Get the Spark jobj associated with objects of various types. |

The use of these functions is illustrated in this (overly) simple example:

```{r}
analyze <- function(x, features) {
  
  # normalize whatever we were passed (e.g. a dplyr tbl) into a Spark DataFrame
  df <- sparkapi_dataframe(x)
  
  # get the underlying connection so we can create new objects
  sc <- sparkapi_connection(df)
  
  # create an object to do the analysis and call its `analyze` and `summary`
  # methods (note that the df and features are passed to the analyze function)
  summary <- sc %>%  
    sparkapi_invoke_new("com.example.tools.Analyzer") %>% 
      sparkapi_invoke("analyze", df, features) %>% 
      sparkapi_invoke("summary")

  # return the results
  summary
}
```

The first argument is an object that can be accessed using the Spark DataFrame API (this might be an actual reference to a DataFrame or could rather be a dplyr `tbl` which has a DataFrame reference inside it). After using the [sparkapi_jobj](reference/sparkapi/latest/sparkapi_jobj.html) function to normalize the reference, we call [sparkapi_connection](reference/sparkapi/latest/sparkapi_connection.html) to extract the underlying Spark connection associated with the data frame. Finally, we create a new `Analyzer` object, call it's `analyze` method with the DataFrame and list of features, and then call the `summary` method on the results of the analysis.

Accepting a jobj (in this case a DataFrame) as the first argument of a function makes it very easy to incorporate into magrittr pipelines so this pattern is highly recommended when possible.

## Dependencies

When creating an extension package you may need to include additional dependencies. Your dependencies might be a set of [Spark Packages](https://spark-packages.org/) or might be a custom JAR file. In either case, you'll need a way to specify that these dependencies be included during the initialization of a Spark session via [spark_connect](reference/sparklyr/latest/spark_connect). A Spark dependency is defined using the `sparkapi_dependency` function:

| Function | Description  |
|----------------------------|---------------------------------------------|
| [`sparkapi_dependency`](reference/sparkapi/latest/sparkapi_dependency.html) | Define a Spark dependency consisting of JAR files and Spark packages. |

Your extension package can specify it's dependencies by implementing a function named `sparkapi_dependencies` within the package (this function should *not* be publicly exported). For example, let's say you were creating an extension package named **sparkds** that needed to include a custom JAR as well as the Redshift and Apache Avro packages:

```{r}
sparkapi_dependencies <- function(config, ...) {
  sparkapi_dependency(
    jars = system.file("java/sparkds.jar", package = "sparkds"),
    packages = c("com.databricks:spark-redshift_2.10:0.6.0",
                 "com.databricks:spark-avro_2.10:2.0.1")
  )
}
```

The `...` argument is unused but nevertheless should be included to ensure continued compatibility if new arguments are added to `sparkapi_dependencies` in the future.

When users connect to a Spark cluster and want to use your extension package within their session they simply include the **sparkds** package in a list of extensions passed to [`spark_connect`](reference/sparklyr/latest/spark_connect.html):

```{r, eval=FALSE}
library(sparklyr)
sc <- spark_connect(master = "local", version = "1.6.1", extensions = c("sparkds"))
```

### CRAN 

When including a JAR file within an R package distributed on CRAN, you should follow the guidelines provided in [Writing R Extensions](https://cran.r-project.org/doc/manuals/r-release/R-exts.html#Non_002dR-scripts-in-packages):

> "Java code is a special case: except for very small programs, .java files should be byte-compiled (to a .class file) and distributed as part of a .jar file: the conventional location for the .jar file(s) is `inst/java`. It is desirable (and required under an Open Source license) to make the Java source files available: this is best done in a top-level `java` directory in the package -- the source files should not be installed."









