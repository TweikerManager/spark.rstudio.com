<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
<title>Index. rspark 0.1.14</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="author" content="">

<link href="css/bootstrap.css" rel="stylesheet">
<link href="css/bootstrap-responsive.css" rel="stylesheet">
<link href="css/highlight.css" rel="stylesheet">
<link href="css/staticdocs.css" rel="stylesheet">

<!--[if lt IE 9]>
  <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
  </head>

  <body>
    <div class="navbar">
  <div class="navbar-inner">
    <div class="container">
      <a class="brand" href="#">rspark 0.1.14</a>
      <div class="nav">
        <ul class="nav">
          <li><a href="index.html"><i class="icon-home icon-white"></i> Index</a></li>
        </ul>
      </div>
    </div>
  </div>
</div>

    <div class="container">
      <header>
        
      </header>
      
      <div class="row">
  <div class="span8">
    <h1>Spark Interface for R</h1>

<p><a href="https://travis-ci.com/rstudio/rspark"><img src="https://travis-ci.com/rstudio/rspark.svg?token=MxiS2SHZy3QzqFf34wQr&amp;branch=master" alt="Travis-CI Build Status"/></a></p>

<p>A set of tools to provision, connect and interface to Apache Spark from within the R language and ecosystem. This package supports connecting to local and remote Apache Spark clusters and provides support for R packages like dplyr and DBI.</p>

<h2>Installation</h2>

<p>You can install the development version of the <strong>rspark</strong> package using <strong>devtools</strong> as follows (note that installation of the development version of <strong>devtools</strong> itself is also required):</p>

<pre><code class="r">devtools::install_github(&quot;hadley/devtools&quot;)
devtools::reload(devtools::inst(&quot;devtools&quot;))

devtools::install_github(&quot;rstudio/rspark&quot;, auth_token = &quot;56aef3d82d3ef05755e40a4f6bdaab6fbed8a1f1&quot;)
</code></pre>

<p>You can then install various versions of Spark using the <code>spark_install</code> function:</p>

<pre><code class="r">library(rspark)
spark_install(version = &quot;1.6.1&quot;, hadoop_version = &quot;2.6&quot;, reset = TRUE)
</code></pre>

<h2>dplyr Interface</h2>

<p>The rspark package implements a dplyr back-end for Spark. Connect to Spark using the <code>spark_connect</code> function then obtain a dplyr interface using <code>src_spark</code> function:</p>

<pre><code class="r"># connect to local spark instance and get a dplyr interface
library(rspark)
library(dplyr)
sc &lt;- spark_connect(&quot;local&quot;, version = &quot;1.6.1&quot;)
db &lt;- src_spark(sc)
</code></pre>

<p>Now we copy a couple of datasets from R into the Spark cluster:</p>

<pre><code class="r"># copy the flights table from the nycflights13 package to Spark
copy_to(db, nycflights13::flights, &quot;flights&quot;)
flights &lt;- tbl(db, &quot;flights&quot;)

# copy the Batting table from the Lahman package to Spark
copy_to(db, Lahman::Batting, &quot;batting&quot;)
batting &lt;- tbl(db, &quot;batting&quot;)

# copy the iris table to Spark
copy_to(db, iris, &quot;iris&quot;)
iris_tbl &lt;- tbl(db, &quot;iris&quot;)
</code></pre>

<p>Then you can run dplyr against Spark:</p>

<pre><code class="r"># filter by departure delay and print the first few records
flights %&gt;% filter(dep_delay == 2)
</code></pre>

<pre><code>## Source:   query [?? x 16]
## Database: spark connection master=local app=rspark local=TRUE
## 
##     year month   day dep_time dep_delay arr_time arr_delay carrier tailnum
##    &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;   &lt;chr&gt;   &lt;chr&gt;
## 1   2013     1     1      517         2      830        11      UA  N14228
## 2   2013     1     1      542         2      923        33      AA  N619AA
## 3   2013     1     1      702         2     1058        44      B6  N779JB
## 4   2013     1     1      715         2      911        21      UA  N841UA
## 5   2013     1     1      752         2     1025        -4      UA  N511UA
## 6   2013     1     1      917         2     1206        -5      B6  N568JB
## 7   2013     1     1      932         2     1219        -6      VX  N641VA
## 8   2013     1     1     1028         2     1350        11      UA  N76508
## 9   2013     1     1     1042         2     1325        -1      B6  N529JB
## 10  2013     1     1     1231         2     1523        -6      UA  N402UA
## ..   ...   ...   ...      ...       ...      ...       ...     ...     ...
## Variables not shown: flight &lt;int&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time
##   &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;.
</code></pre>

<p><a href="https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html">Introduction to dplyr</a> provides additional dplyr examples you can try. For example, consider the last example from the tutorial which plots data on flight delays:</p>

<pre><code class="r">delay &lt;- flights %&gt;% 
  group_by(tailnum) %&gt;%
  summarise(count = n(), dist = mean(distance), delay = mean(arr_delay)) %&gt;%
  filter(count &gt; 20, dist &lt; 2000, !is.na(delay)) %&gt;%
  collect

# plot delays
library(ggplot2)
ggplot(delay, aes(dist, delay)) +
  geom_point(aes(size = count), alpha = 1/2) +
  geom_smooth() +
  scale_size_area(max_size = 2)
</code></pre>

<p><img src="res/ggplot2-1.png" alt=""/></p>

<h3>Window Functions</h3>

<p>dplyr <a href="https://cran.r-project.org/web/packages/dplyr/vignettes/window-functions.html">window functions</a> are also supported, for example:</p>

<pre><code class="r">batting %&gt;%
  select(playerID, yearID, teamID, G, AB:H) %&gt;%
  arrange(playerID, yearID, teamID) %&gt;%
  group_by(playerID) %&gt;%
  filter(min_rank(desc(H)) &lt;= 2 &amp; H &gt; 0)
</code></pre>

<pre><code>## Source:   query [?? x 7]
## Database: spark connection master=local app=rspark local=TRUE
## Groups: playerID
## 
##     playerID yearID teamID     G    AB     R     H
##        &lt;chr&gt;  &lt;int&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
## 1  anderal01   1941    PIT    70   223    32    48
## 2  anderal01   1942    PIT    54   166    24    45
## 3  balesco01   2008    WAS    15    15     1     3
## 4  balesco01   2009    WAS     7     8     0     1
## 5  bandoch01   1986    CLE    92   254    28    68
## 6  bandoch01   1984    CLE    75   220    38    64
## 7  bedelho01   1962    ML1    58   138    15    27
## 8  bedelho01   1968    PHI     9     7     0     1
## 9  biittla01   1977    CHN   138   493    74   147
## 10 biittla01   1975    MON   121   346    34   109
## ..       ...    ...    ...   ...   ...   ...   ...
</code></pre>

<h2>ML Functions</h2>

<p>MLlib functions are also supported, see [ml samples](docs/ml_examples.md). For instasnce, k-means can be run as:</p>

<pre><code class="r">model &lt;- tbl(db, &quot;iris&quot;) %&gt;%
  select(Petal_Width, Petal_Length) %&gt;%
  ml_kmeans(centers = 3)

tbl(db, &quot;iris&quot;) %&gt;%
  select(Petal_Width, Petal_Length) %&gt;%
  collect %&gt;%
  ggplot(aes(Petal_Length, Petal_Width)) +
    geom_point(data = model$centers, aes(Petal_Width, Petal_Length), size = 60, alpha = 0.1) +
    geom_point(aes(Petal_Width, Petal_Length), size = 2, alpha = 0.5)
</code></pre>

<p><img src="res/unnamed-chunk-6-1.png" alt=""/></p>

<h2>EC2</h2>

<p>To start a new 1-master 1-slave Spark cluster in EC2 run the following code:</p>

<pre><code class="r">library(rspark)
ci &lt;- spark_ec2_cluster(access_key_id = &quot;AAAAAAAAAAAAAAAAAAAA&quot;,
                        secret_access_key = &quot;1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1&quot;,
                        pem_file = &quot;spark.pem&quot;)

spark_ec2_deploy(ci)

spark_ec2_web(ci)
spark_ec2_rstudio(ci)

spark_ec2_stop(ci)
spark_ec2_destroy(ci)
</code></pre>

<p>The <code>access_key_id</code>, <code>secret_access_key</code> and <code>pem_file</code> need to be retrieved from the AWS console.</p>

<p>For additional configuration and examples read: [Using RSpark in EC2](docs/ec2.md)</p>

<h2>Extensibility</h2>

<p>Spark provides low level access to native JVM objects, this topic targets users creating packages based on low-level spark integration. Here&#39;s an example of an R <code>count_lines</code> function built by calling Spark functions for reading and counting the lines of a text file.</p>

<pre><code class="r"># define an R interface to Spark line counting
count_lines &lt;- function(sc, path) {
  file &lt;- spark_invoke(sc, &quot;textFile&quot;, path, as.integer(1))
  spark_invoke(file, &quot;count&quot;)
}

# write a CSV 
tempfile &lt;- tempfile(fileext = &quot;.csv&quot;)
write.csv(nycflights13::flights, tempfile, row.names = FALSE, na = &quot;&quot;)

# call spark to count the lines
count_lines(sc, tempfile)
</code></pre>

<pre><code>## [1] 336777
</code></pre>

<p>Package authors can use this mechanism to create an R interface to any of Spark&#39;s underlying Java APIs.</p>

<h2>dplyr Utilities</h2>

<p>You can cache a table into memory with:</p>

<pre><code class="r">tbl_cache(db, &quot;batting&quot;)
</code></pre>

<p>and unload from memory using:</p>

<pre><code class="r">tbl_uncache(db, &quot;batting&quot;)
</code></pre>

<h2>Connection Utilities</h2>

<p>You can view the Spark web console using the <code>spark_web</code> function:</p>

<pre><code class="r">spark_web(sc)
</code></pre>

<p>You can show the log using the <code>spark_log</code> function:</p>

<pre><code class="r">spark_log(sc, n = 10)
</code></pre>

<pre><code>## 16/06/14 11:18:04 INFO ContextCleaner: Cleaned shuffle 11
## 16/06/14 11:18:04 INFO BlockManagerInfo: Removed broadcast_51_piece0 on localhost:54213 in memory (size: 351.0 B, free: 487.0 MB)
## 16/06/14 11:18:04 INFO ContextCleaner: Cleaned accumulator 114
## 16/06/14 11:18:04 INFO BlockManagerInfo: Removed broadcast_50_piece0 on localhost:54213 in memory (size: 1678.0 B, free: 487.0 MB)
## 16/06/14 11:18:04 INFO ContextCleaner: Cleaned accumulator 113
## 16/06/14 11:18:04 INFO Executor: Finished task 0.0 in stage 45.0 (TID 475). 2082 bytes result sent to driver
## 16/06/14 11:18:04 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 475) in 91 ms on localhost (1/1)
## 16/06/14 11:18:04 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool 
## 16/06/14 11:18:04 INFO DAGScheduler: ResultStage 45 (count at NativeMethodAccessorImpl.java:-2) finished in 0.091 s
## 16/06/14 11:18:04 INFO DAGScheduler: Job 31 finished: count at NativeMethodAccessorImpl.java:-2, took 0.093238 s
</code></pre>

<p>Finally, we disconnect from Spark:</p>

<pre><code class="r">spark_disconnect(sc)
</code></pre>

<h2>Additional Resources</h2>

<p>For performance runs under various parameters, read: [RSpark Dplyr Performance](docs/perf_dplyr.md) and [RSpark 1B-Rows Performance](docs/perf_1b.md)</p>


    <h2>Help topics</h2>

    <h3>Connecting to Spark</h3>
    <p>Functions for installing Spark components and connecting to Spark clusters.</p>

    
    <ul class="index">
        
      <li>
        <code><a href="spark_connect.html">spark_connect</a></code><br />Connects to Spark and establishes the Spark Context</li>
    
    </ul>
    <h3>Reading and Writing Data</h3>
    <p>Functions for reading and writing Spark data frames</p>

    
    <ul class="index">

    </ul>
    <h3>Other</h3>
    
    
    <ul class="index">
        
      <li>
        <code><a href="as_spark_dataframe.html">as_spark_dataframe</a></code><br />Convert a tbl to a Spark Dataset.</li>
            
      <li>
        <code><a href="copy_to.src_spark.html">copy_to.src_spark</a></code><br />Copies the source data frame into a Spark table</li>
            
      <li>
        <code><a href="db_data_type.src_spark.html">db_data_type.src_spark</a></code><br />This operation is currently not supported in Spark</li>
            
      <li>
        <code><a href="dbi-spark-table.html">dbi-spark-table</a></code>(dbExistsTable,DBISparkConnection,character-method, dbListTables,DBISparkConnection-method, dbReadTable,DBISparkConnection,character-method, dbRemoveTable,DBISparkConnection,character-method, dbWriteTable,DBISparkConnection-method, mutate_.tbl_spark)<br />DBI Spark Table</li>
            
      <li>
        <code><a href="dbSetProperty.html">dbSetProperty</a></code><br />Sets a property for the connection</li>
            
      <li>
        <code><a href="dbSetProperty-DBISparkConnection-character-character-method.html">dbSetProperty,DBISparkConnection,character,character-method</a></code><br />Sets a property for the connection</li>
            
      <li>
        <code><a href="dplyr-spark-data.html">dplyr-spark-data</a></code>(load_csv, load_json, load_parquet, save_csv, save_json, save_parquet)<br />Loads a CSV file and provides a data source compatible with dplyr</li>
            
      <li>
        <code><a href="dplyr-spark-table.html">dplyr-spark-table</a></code>(collect.tbl_spark, sample_frac.tbl_spark, sample_n.tbl_spark, sql_build.tbl_spark)<br />Dplyr table definitions for Spark</li>
            
      <li>
        <code><a href="ec2-spark.html">ec2-spark</a></code>(spark_ec2_cluster, spark_ec2_deploy, spark_ec2_destroy, spark_ec2_login, spark_ec2_master, spark_ec2_rstudio, spark_ec2_start, spark_ec2_stop, spark_ec2_web)<br />Install an EC2 Spark cluster</li>
            
      <li>
        <code><a href="ml_kmeans.html">ml_kmeans</a></code><br />Computes kmeans from a dplyr source</li>
            
      <li>
        <code><a href="ml_linear_regression.html">ml_linear_regression</a></code><br />Linear regression from a dplyr source</li>
            
      <li>
        <code><a href="ml_logistic_regression.html">ml_logistic_regression</a></code><br />Logistic regression from a dplyr source</li>
            
      <li>
        <code><a href="ml_pca.html">ml_pca</a></code><br />Perform Principal Components Analaysis using spark.ml</li>
            
      <li>
        <code><a href="ml_random_forest.html">ml_random_forest</a></code><br />Random Forests with Spark ML</li>
            
      <li>
        <code><a href="partition.html">partition</a></code><br />Partition a Spark Dataframe</li>
            
      <li>
        <code><a href="print.jobj.html">print.jobj</a></code><br />Print a JVM object reference.</li>
            
      <li>
        <code><a href="print.src_spark.html">print.src_spark</a></code><br />Prints information associated to the dplyr source</li>
            
      <li>
        <code><a href="spark_can_install.html">spark_can_install</a></code><br />Check if Spark can be installed in this system</li>
            
      <li>
        <code><a href="spark_config.html">spark_config</a></code><br />Defines a configuration file based on the config package and built-in defaults</li>
            
      <li>
        <code><a href="spark_connection_app_name.html">spark_connection_app_name</a></code><br />Retrieves the application name from a Spark Connection</li>
            
      <li>
        <code><a href="spark_connection_is_local.html">spark_connection_is_local</a></code><br />TRUE if the Spark Connection is a local install</li>
            
      <li>
        <code><a href="spark_connection_is_open.html">spark_connection_is_open</a></code><br />Checks to see if the connection into Spark is still open</li>
            
      <li>
        <code><a href="spark_connection_local_cores.html">spark_connection_local_cores</a></code><br />Number of cores available in the local install</li>
            
      <li>
        <code><a href="spark_connection_on_reconnect.html">spark_connection_on_reconnect</a></code><br />Provides an extension mechanism to allow package builders to support spark_connect(reconnect = TRUE)</li>
            
      <li>
        <code><a href="spark_context.html">spark_context</a></code><br />Retrieves the SparkContext reference from a Spark Connection</li>
            
      <li>
        <code><a href="spark_context_master.html">spark_context_master</a></code>(spark_connection_master)<br />Retrieves master from a Spark Connection</li>
            
      <li>
        <code><a href="spark_dataframe_collect.html">spark_dataframe_collect</a></code><br />Read a Spark Dataset into R.</li>
            
      <li>
        <code><a href="spark_dataframe_split.html">spark_dataframe_split</a></code><br />Split a Spark DataFrame</li>
            
      <li>
        <code><a href="spark_disconnect.html">spark_disconnect</a></code><br />Disconnects from Spark and terminates the running application</li>
            
      <li>
        <code><a href="spark_disconnect_all.html">spark_disconnect_all</a></code><br />Closes all existing connections. Returns the total of connections closed.</li>
            
      <li>
        <code><a href="spark_install.html">spark_install</a></code><br />Provides support to download and install the given Spark version</li>
            
      <li>
        <code><a href="spark_install_available.html">spark_install_available</a></code><br />Check if the given Spark version is available in this system</li>
            
      <li>
        <code><a href="spark_install_tar.html">spark_install_tar</a></code><br />Provides support to install a version of Spark from a given TAR file</li>
            
      <li>
        <code><a href="spark_invoke.html">spark_invoke</a></code><br />Executes a method on the given object</li>
            
      <li>
        <code><a href="spark_invoke_static.html">spark_invoke_static</a></code><br />Executes an static method on the given object</li>
            
      <li>
        <code><a href="spark_invoke_static_ctor.html">spark_invoke_static_ctor</a></code><br />Executes an static method on the given object</li>
            
      <li>
        <code><a href="spark_log.html">spark_log</a></code>(print.spark_log, spark_log_file)<br />Retrieves the last n entries in the Spark log</li>
            
      <li>
        <code><a href="spark_sql.html">spark_sql</a></code><br />Executes arbitrary SQL statements</li>
            
      <li>
        <code><a href="spark_versions.html">spark_versions</a></code><br />Retrieves available versions of Spark</li>
            
      <li>
        <code><a href="spark_versions_info.html">spark_versions_info</a></code><br />Retrieves component information for the given Spark and Hadoop versions</li>
            
      <li>
        <code><a href="spark_web.html">spark_web</a></code><br />Opens the Spark web interface</li>
            
      <li>
        <code><a href="spark-transactions.html">spark-transactions</a></code>(dbBegin,DBISparkConnection-method, dbCommit,DBISparkConnection-method, dbRollback,DBISparkConnection-method)<br />Spark Transactions.</li>
            
      <li>
        <code><a href="sql_analyze.src_spark.html">sql_analyze.src_spark</a></code><br />This operation is currently not supported in Spark</li>
            
      <li>
        <code><a href="sql_begin.src_spark.html">sql_begin.src_spark</a></code><br />This operation is currently not supported in Spark</li>
            
      <li>
        <code><a href="sql_commit.src_spark.html">sql_commit.src_spark</a></code><br />This operation is currently not supported in Spark</li>
            
      <li>
        <code><a href="sql_create_index.src_spark.html">sql_create_index.src_spark</a></code><br />This operation is currently not supported in Spark</li>
            
      <li>
        <code><a href="sql_create_table.src_spark.html">sql_create_table.src_spark</a></code><br />This operation is currently not supported in Spark</li>
            
      <li>
        <code><a href="sql_drop_table.src_spark.html">sql_drop_table.src_spark</a></code><br />Removes a Spark table</li>
            
      <li>
        <code><a href="sql_insert_into.src_spark.html">sql_insert_into.src_spark</a></code><br />This operation is currently not supported in Spark</li>
            
      <li>
        <code><a href="sql_rollback.src_spark.html">sql_rollback.src_spark</a></code><br />This operation is currently not supported in Spark</li>
            
      <li>
        <code><a href="src_context.html">src_context</a></code><br />Retrieves the Spark connection object from a given dplyr src</li>
            
      <li>
        <code><a href="src_spark.html">src_spark</a></code><br />Connect to Spark for Dplyr.</li>
            
      <li>
        <code><a href="tbl_cache.html">tbl_cache</a></code><br />Loads a table into memory</li>
            
      <li>
        <code><a href="tbl_uncache.html">tbl_uncache</a></code><br />Unloads table from memory</li>
    
    </ul>
  </div>
  
  <div class="span3 offset1">
        
    <h2>Dependencies</h2>
    <ul>
      
      <li><strong>Imports</strong>: methods, dplyr, DBI, assertthat, rappdirs, lazyeval, config, yaml</li>
      <li><strong>Suggests</strong>: testthat, reshape2, RCurl</li>
      
    </ul>
    <h2>Authors</h2>
    <ul>
      <li><a href="mailto:javier@rstudio.com">Javier Luraschi</a> [aut, cre]</li>
      <li><a href="mailto:kevin@rstudio.com">Kevin Ushey</a> [aut]</li>
      <li><a href="mailto:jj@rstudio.com">JJ Allaire</a> [aut]</li>
      <li>RStudio [cph]</li>
    </ul>

  </div>
</div>
      
      <footer>
      <p class="pull-right"><a href="#">Back to top</a></p>
<p>Built by <a href="https://github.com/hadley/staticdocs">staticdocs</a>. Styled with <a href="https://getbootstrap.com/2.0.4/">bootstrap</a>.</p>
      </footer>
    </div>
  </body>
</html>