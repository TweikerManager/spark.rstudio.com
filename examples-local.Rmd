---
title: "Using Spark locally"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
---
## Introduction 

**sparklyr** makes it easy to start using and learning **Spark**. The package enables the installation and interaction of a Spark locally in a single computer, including on  computers with the Microsoft Windows OS.  

This article focuses on some of the main configuration settings, commands and command arguments that can be used to optimize an analysis developed in **Spark** in the *local* mode.

```{r, include=FALSE}
library(htmltools)

thumbnail <- function(title, img, href="", caption = TRUE, size = 6) {
  div(class = paste("col-sm-",size, sep=""),
      a(class = "thumbnail", title = title, href = href,
        img(src = img),
        div(class = ifelse(caption, "caption", ""),
          ifelse(caption, title, ""), align="center")))}

```

## Preparation

### Download Test Data

The 2008 and 2007 Flights data from the Statistical Computing site will be used for this exercise.  The **spark_read_csv** supports reading compressed CSV files in a **bz2** format, so no additional file preparation is needed.

```{r, eval=FALSE}
if(!file.exists("2008.csv.bz2"))
  {download.file("http://stat-computing.org/dataexpo/2009/2008.csv.bz2", "2008.csv.bz2")}
if(!file.exists("2007.csv.bz2"))
  {download.file("http://stat-computing.org/dataexpo/2009/2007.csv.bz2", "2007.csv.bz2")}
```

### Install Spark Locally

**sparklyr** can download the appropriate Spark files to run any supported version.  Version **2.0.0** will be used in this exercise.

```{r, eval=FALSE}
library(sparklyr)
library(dplyr)
library(ggplot2)
spark_install(version="2.0.0")
```

## Start Spark Session

### On Windows

If on a Microsoft Windows computer, set the Java Home prior to connecting.  The path may be different that the example below:

```{r, eval=FALSE}
java_path <- normalizePath('C:/Program Files/Java/jre1.8.0_66')
Sys.setenv(JAVA_HOME=java_path)
```

### Main Configuration Setting

In 'Local' mode, the main configuration we need to modify is the **sparklyr.shell.driver-memory**.  The amount of memory requested depends on the capacity of the computer, in this case 16 Gigabytes are being requested.

```{r, eval=FALSE}
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "16G"
##--- Tip: Specifying 'version' when connecting will reduce issues when multiple Spark versions are installed locally
sc <- spark_connect(master="local", config = conf, version="2.0.0")
```

## Spark Read

### The Memory Argument

The **memory** argument controls if the data will be loaded into memory as an RDD.  Setting it to **FALSE** means that Spark will essentially map the file, but not make a copy of it in memory.  This makes the **spark_read_csv** command run faster, but the trade off is that any data transformation operations will take much longer.

```{r, eval=FALSE}
spark_read_csv(sc, "flights_spark_2008","2008.csv.bz2", memory = FALSE)
```

In the RStudio IDE, the **flights_spark_2008** table now shows up in the Spark tab. 

<center><a href="images/examples-local/latest-tab1.png">
  <img src="images/examples-local/latest-tab1.png" , width="400px"/>
</a></center>

To access the Spark Web UI, click the Spark UI button in the RStudio Spark Tab. As expected, the **Storage** page shows no tables loaded into memory.

<center><a href="images/examples-local/storage-1.png">
  <img src="images/examples-local/storage-1.png" , width="1000px"/>
</a></center>

### Where is the requested Driver Memory?

In the **Executors** page we can see that the Storage Memory is at about half of the 16 gigabytes requested.  This is mainly because of a Spark setting called **spark.memory.fraction**, which reserves by default 40% of the memory requested.

<center><a href="images/examples-local/drivers-1.png">
  <img src="images/examples-local/drivers-1.png", width="1000px"/>
</a></center>

## Loading Less Data into Memory

Using the pre-processing capabilities of Spark, the data will be transformed before being loaded into memory.  In this section, we will continue to build on the example started in the **Spark Read** section

### Lazy Transform

The following **dplyr** script will not be immediately run, so the code is processed quickly.  There are some check-ups made, but for the most part it is building a Spark SQL statement in the background. 

```{r, eval=FALSE}
flights_table <- tbl(sc,"flights_spark_2008") %>%
  mutate(DepDelay = as.numeric(DepDelay),
         ArrDelay = as.numeric(ArrDelay),
         DepDelay > 15 , DepDelay < 240,
         ArrDelay > -60 , ArrDelay < 360, 
         Gain = DepDelay - ArrDelay) %>%
  filter(ArrDelay>0) %>%
  select(Origin, Dest, UniqueCarrier, Distance, DepDelay, ArrDelay, Gain)
```

### Register in Spark

**sdf_register** will register the resulting Spark SQL in Spark.  The results will show up as a table called **flights_spark**.  But a table of the same name is still not loaded into memory in Spark. 

```{r, eval=FALSE}
sdf_register(flights_table, "flights_spark")
```

<center><a href="images/examples-local/spark-tab-3.png">
  <img src="images/examples-local/spark-tab-3.png", width="400px"/>
</a></center>

### Cache into Memory

The **tbl_cache** command loads the results into an Spark RDD in memory, so any analysis from there on will not need to re-read and re-transform the original file.  The resulting Spark RDD is smaller than the original file because the transformations created a smaller data set than the original file.  

```{r, eval=FALSE}
tbl_cache(sc, "flights_spark")
```

<center><a href="images/examples-local/storage-new-3.png">
  <img src="images/examples-local/storage-new-3.png", width="1000px"/>
</a></center>


## Process on the fly

The plan is to read the Flights 2007 file, combine it with the 2008 file and summarize the data without bringing either file fully into memory.

```{r, eval=FALSE}
spark_read_csv(sc, "flights_spark_2007","2007.csv.bz2", memory = FALSE)
```


### Union and Transform

The **union** command is akin to the **bind_rows**.  It will allow us to append the 2007 file to the 2008 file.  As with the previous transform, this script will be evaluated lazily.

```{r, eval=FALSE}
all_flights <- tbl(sc,"flights_spark_2008") %>%
  union(tbl(sc,"flights_spark_2007")) %>%
  mutate(flight_date = paste(Year,Month,DayofMonth,sep="-"),
         days_since = datediff(current_date(), flight_date)) %>%
  group_by(flight_date,days_since) %>%
  tally() %>%
  arrange(-days_since) 
```

### Collect into R

When receiving a **collect** command, Spark will execute the SQL statement and send the results back to R in a data frame.  In this case, R only loads 731 rows into a data frame called all_flights.

```{r, eval=FALSE}
all_flights <- all_flights %>%
  collect()
```

<center><a href="images/examples-local/all-flights.png">
  <img src="images/examples-local/all-flights.png", width="400px"/>
</a></center>

### Plot in R

Now the smaller data set can be plotted 

```{r, eval=FALSE}
ggplot(data=all_flights, aes(x=days_since, y=n)) +
  geom_line()
```

<center><a href="images/examples-local/new-plot.png">
  <img src="images/examples-local/new-plot.png", width="600px"/>
</a></center>
