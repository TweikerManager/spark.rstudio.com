---
title: "Maching learning with Spark ML"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

## Overview

You can orchestrate machine learning algorithms in a Spark cluster via specific functions in `sparklyr`. These functions connect to a set of high-level APIs built on top of DataFrames that help you create and tune machine learning pipelines. 

The following machine learning algorithms are supported in `sparklyr`:

* Classification 
    * Logistic regression
    * Multilayer perceptron classifier (MLPC)
    * Random forest classifier
    * Gradient-boosted tree classifier
    * One vs rest classifier
    * Naive Bayes
* Regression
    * Linear regression
    * Random forest regression
    * Gradient-boosted tree regression 
    * Survival regression
* Clustering
    * K-means
    * Latent Dirichlet allocation (LDA)
* Dimensionality reduction
    * Pricipal components analysis (PCA)

You can also use specific functions in `sparklyr` to prepare data via the machine learning APIs. These functions help you extract, transform, and subset your data as part of an analytic pipeline.

* Extraction: Extracting features from “raw” data
* Transformation: Scaling, converting, or modifying features
* Selection: Selecting a subset from a larger set of features

Note that `dplyr` connects to the Spark SQL API, whereas these `sparklyr` functions connect to the Spark Machine Learning APIs.

## Details

Spark provides two main APIs for machine learning -- the newer, more modern API called [Spark.ML](http://spark.apache.org/docs/latest/ml-guide.html) built on top of Spark DataFrames, and the older API called [Spark.MLlib](http://spark.apache.org/docs/latest/mllib-guide.html) built on top of RDD data structures . We target the newer interface. 

Note that all of the routines we export should work in Spark 1.6.1, but in some cases some model fits don't provide as much information as in Spark 2.0.0 (e.g. in Spark 1.6.1 k-means doesn't provide the 'explained variance' for each component).

****

## Concepts

`sparklyr` functions that connect to the machine learning Spark APIs fall into three categories.

- Machine learning algorithms for analyzing data (`ml_*`)
- Feature transformers for manipulating individual features (`ft_*`)
- Verbs for manipulating DataFrames (`sdf_*`)

In this example we'll use the `mtcars` data set to build a model. First we will initialize the Spark connection and copy the data into Spark.

```{r}
library(sparklyr)
library(ggplot2)
library(dplyr)
sc <- spark_connect("local", version = "1.6.1")
mtcars_df <- copy_to(sc, mtcars, "mtcars", overwrite = TRUE)
```

We will then use a linear model to predict fuel consumption (mpg) as a function of weight (wt) and cylindars (cyl).

```{r}
(fit <- ml_linear_regression(mtcars_df, "mpg", c("wt", "cyl")))
```
```
Call: mpg ~ wt + cyl

Coefficients:
(Intercept)          wt         cyl 
  39.686261   -3.190972   -1.507795 
```

We can also apply standard function methods in R to the fitted model.

```{r, eval=FALSE}
summary(fit)
coefficients(fit)
residuals(fit)
predict(fit, mtcars_df)
```

Typically, data analysis requires data preprocessing. We can transform individual features by calling them inside of `sdf_mutate`. These routines generally take one or more input columns and generate a new output column formed as a transformation of those columns. A simple example is the binarizer, which transforms a feature into 1's and 0's based on a threshold. For example, we can set compare 8 cylindar vehicles to all other types of vehicles with the following:

```{r}
sdf_mutate(mtcars_df, cyl8 = ft_binarizer(cyl, 8))
```

It's important to note that the Spark DataFrame verbs (including sdf_mutate in the example here) compute their results eagerly whereas dplyr verbs compute their results lazily. In the example below, we combine Spark DataFrame verbs and dplyr verbs in the same chain.

```{r, eval=FALSE}
mtcars_df %>%
  mutate(weight = weight * 1000) %>%
  sdf_mutate(mtcars_df, cyl8 = ft_binarizer(cyl, 6))
```

You can partion data into training and testing subsets with the `sdf_partition` verb.

```{r}
mtcars_df_part <- sdf_partition(mtcars_df, training=0.6, test=0.4)
fit_training <- ml_linear_regression(mtcars_df_part$training, "mpg", c("wt", "cyl"))
predict(fit_training, mtcars_df_part$test)
```

****

## Examples

### Initialization

```{r, eval=FALSE}
library(sparklyr)
library(ggplot2)
library(dplyr)
sc <- spark_connect("local", version = "1.6.1")
iris_tbl <- copy_to(sc, iris, "iris", overwrite = TRUE)
iris_tbl
```
```
Source:   query [?? x 5]
Database: spark connection master=local app=sparklyr local=TRUE

   Sepal_Length Sepal_Width Petal_Length Petal_Width Species
          <dbl>       <dbl>        <dbl>       <dbl>   <chr>
1           5.1         3.5          1.4         0.2  setosa
2           4.9         3.0          1.4         0.2  setosa
3           4.7         3.2          1.3         0.2  setosa
4           4.6         3.1          1.5         0.2  setosa
5           5.0         3.6          1.4         0.2  setosa
6           5.4         3.9          1.7         0.4  setosa
7           4.6         3.4          1.4         0.3  setosa
8           5.0         3.4          1.5         0.2  setosa
9           4.4         2.9          1.4         0.2  setosa
10          4.9         3.1          1.5         0.1  setosa
# ... with more rows
```
*Note that the names of variables within the have been transformed (replacing `.` with `_`) to work around an issue in the Spark 2.0.0-preview used in constructing this document -- we expect the issue to be resolved with the release of Spark 2.0.0.*

### KMeans 

```{r}
model <- iris_tbl %>%
  select(Petal_Width, Petal_Length) %>%
  ml_kmeans(centers = 3)

iris_tbl %>%
  select(Petal_Width, Petal_Length) %>%
  collect %>%
  ggplot(aes(Petal_Length, Petal_Width)) +
    geom_point(data = model$centers, aes(Petal_Width, Petal_Length), size = 60, alpha = 0.1) +
    geom_point(aes(Petal_Width, Petal_Length), size = 2, alpha = 0.5)
```

![](images/ml_kmeans.png)

### Linear Regression

```{r}
model <- iris_tbl %>%
  select(Petal_Width, Petal_Length) %>%
  ml_linear_regression(response = "Petal_Length", features = c("Petal_Width"))

iris_tbl %>%
  select(Petal_Width, Petal_Length) %>%
  collect %>%
  ggplot(aes(Petal_Length, Petal_Width)) +
    geom_point(aes(Petal_Width, Petal_Length), size = 2, alpha = 0.5) +
    geom_abline(aes(slope = coef(model)[["Petal_Width"]],
                    intercept = coef(model)[["(Intercept)"]],
                    color = "red"))
```

![](images/ml_linreg.png)

### Logistic Regression

```{r}
# Prepare beaver dataset
beaver <- beaver2
beaver$activ <- factor(beaver$activ, labels = c("Non-Active", "Active"))
copy_to(sc, beaver, "beaver")
beaver_tbl <- tbl(sc, "beaver")

model <- beaver_tbl %>%
  mutate(response = as.numeric(activ == "Active")) %>%
  ml_logistic_regression(response = "response", features = "temp")

print(model)
```
```
Source:   query [?? x 4]
Database: spark connection master=local app=sparklyr local=TRUE

     day  time  temp      activ
   <dbl> <dbl> <dbl>      <chr>
1    307   930 36.58 Non-Active
2    307   940 36.73 Non-Active
3    307   950 36.93 Non-Active
4    307  1000 37.15 Non-Active
5    307  1010 37.23 Non-Active
6    307  1020 37.24 Non-Active
7    307  1030 37.24 Non-Active
8    307  1040 36.90 Non-Active
9    307  1050 36.95 Non-Active
10   307  1100 36.89 Non-Active
# ... with more rows
Call: response ~ temp

Coefficients:
(Intercept)        temp 
 -550.52331    14.69184 
 ```

### Partitioning 

```{r}
partitions <- tbl(sc, "iris") %>%
  sdf_partition(training = 0.75, test = 0.25, seed = 1099)

fit <- partitions$training %>%
  ml_linear_regression(response = "Petal_Length", features = c("Petal_Width"))

predict(fit, partitions$test)
```
```
 [1] 1.744571 1.521752 1.521752 1.521752 1.298933 1.521752 2.413029 1.521752 1.967390 1.521752 1.521752 1.521752 1.521752 3.527124 3.972763 3.972763 3.749944 5.309678 4.418401 5.086858 4.641220 4.418401 4.641220 4.195582 5.086858 3.972763 5.309678 5.086858 3.972763 5.532497 5.532497 3.972763 4.195582 6.423773 6.200954 4.641220 5.086858
```

### Principal Components Analysis 

```{r}
model <- tbl(sc, "iris") %>%
  select(-Species) %>%
  ml_pca()
print(model)
```
```
Explained variance:
[not available in this version of Spark]

Rotation:
                     PC1         PC2         PC3        PC4
Sepal_Length -0.36138659 -0.65658877  0.58202985  0.3154872
Sepal_Width   0.08452251 -0.73016143 -0.59791083 -0.3197231
Petal_Length -0.85667061  0.17337266 -0.07623608 -0.4798390
Petal_Width  -0.35828920  0.07548102 -0.54583143  0.7536574
```

### Random Forests

```{r}
mForest <- iris_tbl %>%
  ml_random_forest(
    response = "Species",
    features = c("Petal_Length", "Petal_Width"),
    max.bins = 32L,
    max.depth = 5L,
    num.trees = 20L
  )
mPredict <- predict(mForest, iris_tbl)
head(mPredict)
```
```
[1] "setosa" "setosa" "setosa" "setosa" "setosa" "setosa"
```

