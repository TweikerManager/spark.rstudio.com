---
title: "Maching learning with Spark ML"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

## Overview

You can orchestrate machine learning algorithms in a Spark cluster via specific functions in `sparklyr`. These functions connect to a set of high-level APIs built on top of DataFrames that help you create and tune machine learning workflows. 

The ML package contains **Estimators** and **Transformers**. 

An **Estimator** is an algorithm which can be fit on a DataFrame to produce a Transformer. The following machine learning algorithms are supported in `sparklyr`:

* Classification 
    * Logistic regression
    * Multilayer perceptron classifier (MLPC)
    * Random forest classifier
    * Gradient-boosted tree classifier
    * One vs rest classifier
    * Naive Bayes
* Regression
    * Linear regression
    * Random forest regression
    * Gradient-boosted tree regression 
    * Survival regression
* Clustering
    * K-means
    * Latent Dirichlet allocation (LDA)
* Dimensionality reduction
    * Pricipal components analysis (PCA)

A **Transformer** is an algorithm which can transform one DataFrame into another DataFrame. Transformers roughly dived into these groups:

* Extraction: Extracting features from “raw” data
* Transformation: Scaling, converting, or modifying features
* Selection: Selecting a subset from a larger set of features

Note that a learning algorithm itself (i.e. a fitted model) is also a Transformer. This is because a learning algorithm can be used to transform data into predicted values.


## Details

Spark provides two main APIs for machine learning -- the newer, more modern API called [Spark.ML](http://spark.apache.org/docs/latest/ml-guide.html) built on top of Spark DataFrames, and the older API called [Spark.MLlib](http://spark.apache.org/docs/latest/mllib-guide.html) built on top of RDD data structures . We target the newer interface. 

`sparklyr` functions that connect to the machine learning Spark APIs fall into three categories.

- Machine learning algorithms for analyzing data (`ml_*`)
- Feature transformers for manipulating individual features (`ft_*`)
- Functions (Verbs?) for manipulating DataFrames (`sdf_*`)

*Note that all of the routines we export should work in Spark 1.6.1, but in some cases some model fits don't provide as much information as in Spark 2.0.0 (e.g. in Spark 1.6.1 k-means doesn't provide the 'explained variance' for each component).*

*Note that `dplyr` connects to the Spark SQL API, whereas these `sparklyr` functions connect to the Spark Machine Learning APIs.*

****

## Estimators

In this example we'll use the `mtcars` data set to build a model. First we will initialize the Spark connection and copy the data into Spark.

```{r}
library(sparklyr)
library(ggplot2)
library(dplyr)
sc <- spark_connect("local", version = "1.6.1")
mtcars_df <- copy_to(sc, mtcars, "mtcars", overwrite = TRUE)
```

We will then use a linear model to predict fuel consumption (mpg) as a function of weight (wt) and cylindars (cyl).

```{r}
(fit <- ml_linear_regression(mtcars_df, "mpg", c("wt", "cyl")))
```
```
Call: mpg ~ wt + cyl

Coefficients:
(Intercept)          wt         cyl 
  39.686261   -3.190972   -1.507795 
```

We can also apply standard function methods in R to the fitted model.

```{r, eval=FALSE}
summary(fit)
coefficients(fit)
residuals(fit)
predict(fit, mtcars_df)
```

****

## Transformers

Typically, data analysis requires data preprocessing. We can transform individual features by calling `ft_*` inside of `sdf_mutate`. These routines generally take one or more input columns and generate a new output column formed as a transformation of those columns.

Transformers are required to create a new DataFrame after every operation. Use `sdf_register()` to output the DataFrame and define its name. 

A simple example is the binarizer, which transforms a feature into 1's and 0's based on a threshold. For example, we can set compare 8 cylindar vehicles to all other types of vehicles with the following:

```{r}
sdf_mutate(mtcars_df, cyl8 = ft_binarizer(cyl, 6)) %>%
  sdf_register("mtcars_df_mutated")
```

It's important to note that the Spark DataFrame commands (including sdf_mutate in the example here) compute their results eagerly whereas dplyr commands compute their results lazily. In the example below, we combine Spark DataFrame commands and commands into the same chain.

```{r, eval=FALSE}
mtcars_df %>%
  mutate(wt = wt * 1000) %>%
  sdf_mutate(cyl8 = ft_binarizer(cyl, 6)) %>%
  sdf_register("mtcars_df_mutated")
```
```
Source:   query [?? x 12]
Database: spark connection master=local app=sparklyr local=TRUE

     mpg   cyl  disp    hp  drat  qsec    vs    am  gear  carb    wt  cyl8
   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
1   21.0     6 160.0   110  3.90 16.46     0     1     4     4  2620     0
2   21.0     6 160.0   110  3.90 17.02     0     1     4     4  2875     0
3   22.8     4 108.0    93  3.85 18.61     1     1     4     1  2320     0
4   21.4     6 258.0   110  3.08 19.44     1     0     3     1  3215     0
5   18.7     8 360.0   175  3.15 17.02     0     0     3     2  3440     1
6   18.1     6 225.0   105  2.76 20.22     1     0     3     1  3460     0
7   14.3     8 360.0   245  3.21 15.84     0     0     3     4  3570     1
8   24.4     4 146.7    62  3.69 20.00     1     0     4     2  3190     0
9   22.8     4 140.8    95  3.92 22.90     1     0     4     2  3150     0
10  19.2     6 167.6   123  3.92 18.30     1     0     4     4  3440     0
# ... with more rows
```

You can partion data into training and testing subsets with the `sdf_partition` verb.

```{r}
mtcars_df_part <- sdf_partition(mtcars_df, training=0.6, test=0.4)
fit_training <- ml_linear_regression(mtcars_df_part$training, "mpg", c("wt", "cyl"))
predict(fit_training, mtcars_df_part$test)
```

****

## Examples

We will use the `iris` data set to examine a handful of learning altogithms and transformers. The iris data set measures attributes for 150 flowers in 3 different species of iris.

```{r, eval=FALSE}
library(sparklyr)
library(ggplot2)
library(dplyr)
sc <- spark_connect("local", version = "1.6.1")
iris_tbl <- copy_to(sc, iris, "iris", overwrite = TRUE)
iris_tbl
```
```
Source:   query [?? x 5]
Database: spark connection master=local app=sparklyr local=TRUE

   Sepal_Length Sepal_Width Petal_Length Petal_Width Species
          <dbl>       <dbl>        <dbl>       <dbl>   <chr>
1           5.1         3.5          1.4         0.2  setosa
2           4.9         3.0          1.4         0.2  setosa
3           4.7         3.2          1.3         0.2  setosa
4           4.6         3.1          1.5         0.2  setosa
5           5.0         3.6          1.4         0.2  setosa
6           5.4         3.9          1.7         0.4  setosa
7           4.6         3.4          1.4         0.3  setosa
8           5.0         3.4          1.5         0.2  setosa
9           4.4         2.9          1.4         0.2  setosa
10          4.9         3.1          1.5         0.1  setosa
# ... with more rows
```
*Note that the names of variables within the have been transformed (replacing `.` with `_`) to work around an issue in the Spark 2.0.0-preview used in constructing this document -- we expect the issue to be resolved with the release of Spark 2.0.0.*

### KMeans 

```{r}
model <- iris_tbl %>%
  select(Petal_Width, Petal_Length) %>%
  ml_kmeans(centers = 3)

iris_tbl %>%
  select(Petal_Width, Petal_Length) %>%
  collect %>%
  ggplot(aes(Petal_Length, Petal_Width)) +
    geom_point(data = model$centers, aes(Petal_Width, Petal_Length), size = 60, alpha = 0.1) +
    geom_point(aes(Petal_Width, Petal_Length), size = 2, alpha = 0.5)
```

![](images/ml_kmeans.png)

### Linear Regression

```{r}
model <- iris_tbl %>%
  select(Petal_Width, Petal_Length) %>%
  ml_linear_regression(response = "Petal_Length", features = c("Petal_Width"))

iris_tbl %>%
  select(Petal_Width, Petal_Length) %>%
  collect %>%
  ggplot(aes(Petal_Length, Petal_Width)) +
    geom_point(aes(Petal_Width, Petal_Length), size = 2, alpha = 0.5) +
    geom_abline(aes(slope = coef(model)[["Petal_Width"]],
                    intercept = coef(model)[["(Intercept)"]],
                    color = "red"))
```

![](images/ml_linreg.png)

### Logistic Regression

```{r}
# Prepare beaver dataset
beaver <- beaver2
beaver$activ <- factor(beaver$activ, labels = c("Non-Active", "Active"))
copy_to(sc, beaver, "beaver")
beaver_tbl <- tbl(sc, "beaver")

model <- beaver_tbl %>%
  mutate(response = as.numeric(activ == "Active")) %>%
  ml_logistic_regression(response = "response", features = "temp")

print(model)
```
```
Source:   query [?? x 4]
Database: spark connection master=local app=sparklyr local=TRUE

     day  time  temp      activ
   <dbl> <dbl> <dbl>      <chr>
1    307   930 36.58 Non-Active
2    307   940 36.73 Non-Active
3    307   950 36.93 Non-Active
4    307  1000 37.15 Non-Active
5    307  1010 37.23 Non-Active
6    307  1020 37.24 Non-Active
7    307  1030 37.24 Non-Active
8    307  1040 36.90 Non-Active
9    307  1050 36.95 Non-Active
10   307  1100 36.89 Non-Active
# ... with more rows
Call: response ~ temp

Coefficients:
(Intercept)        temp 
 -550.52331    14.69184 
 ```

### Partitioning 

```{r}
partitions <- tbl(sc, "iris") %>%
  sdf_partition(training = 0.75, test = 0.25, seed = 1099)

fit <- partitions$training %>%
  ml_linear_regression(response = "Petal_Length", features = c("Petal_Width"))

predict(fit, partitions$test)
```
```
 [1] 1.744571 1.521752 1.521752 1.521752 1.298933 1.521752 2.413029 1.521752 1.967390 1.521752 1.521752 1.521752 1.521752 3.527124 3.972763 3.972763 3.749944 5.309678 4.418401 5.086858 4.641220 4.418401 4.641220 4.195582 5.086858 3.972763 5.309678 5.086858 3.972763 5.532497 5.532497 3.972763 4.195582 6.423773 6.200954 4.641220 5.086858
```

### Principal Components Analysis 

```{r}
model <- tbl(sc, "iris") %>%
  select(-Species) %>%
  ml_pca()
print(model)
```
```
Explained variance:
[not available in this version of Spark]

Rotation:
                     PC1         PC2         PC3        PC4
Sepal_Length -0.36138659 -0.65658877  0.58202985  0.3154872
Sepal_Width   0.08452251 -0.73016143 -0.59791083 -0.3197231
Petal_Length -0.85667061  0.17337266 -0.07623608 -0.4798390
Petal_Width  -0.35828920  0.07548102 -0.54583143  0.7536574
```

### Random Forests

```{r}
mForest <- iris_tbl %>%
  ml_random_forest(
    response = "Species",
    features = c("Petal_Length", "Petal_Width"),
    max.bins = 32L,
    max.depth = 5L,
    num.trees = 20L
  )
mPredict <- predict(mForest, iris_tbl)
head(mPredict)
```
```
[1] "setosa" "setosa" "setosa" "setosa" "setosa" "setosa"
```

