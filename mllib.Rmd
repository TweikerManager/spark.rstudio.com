---
title: "Maching learning with MLlib"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

## Overview

Just for reference, Spark provides two main APIs for machine learning -- the newer, more modern API called Spark.ML (http://spark.apache.org/docs/latest/ml-guide.html) built on top of Spark DataFrames, and the older API called Spark.MLlib built on top of RDD data structures (http://spark.apache.org/docs/latest/mllib-guide.html). We target the newer interface as it's both easier to use, and we expect it to be more actively maintained going forward for the Spark team. Note that all of the routines we export should work in Spark 1.6.1, but in some cases some model fits don't provide as much information as in Spark 2.0.0 (e.g. in Spark 1.6.1 k-means doesn't provide the 'explained variance' for each component).

Right now, all of the regression models are of the form e.g.

```r
ml_*(x, response, features, <extra params>)
```

replacing '*' with the name of the regression / classification routine, and with 'response' being omitted in the case of classification routines (e.g. k-means). Extra parameters are available as appropriate; e.g. random forest regression allows you to tweak the number of trees, the tree depth, and so on.

Using this interface, you can generally write code like e.g.:


```{r}
 fit <- tbl %>%
  mutate(binary_response = response_var > 10) %>%
  ml_logistic_regression(response = "binary_response", 
                         features = c("feature_1", "feature_2"))
```

It's a fairly 'low-level' interface; accepting things like formula model specifications or dplyr select statements is something we'll hopefully do in the future, but we might not get to it before useR.

Model fits can generally be used in a similar way to R model fits -- e.g.

```{r}
coefficients(fit)
residuals(fit)
predict(fit, newdata = df)
```

Note that 'predict' should accept both raw R data.frames, dplyr tbls and also Spark DataFrames / tbl_spark.

Depending on the model fit, different components might be available -- you can generally see what's available just by inspecting the autocompletion results with '$' (anything that we know will be 'small' will be returned to R eagerly; e.g. scalars describing the model fit). I'll try to amend the documentation to describe the return values as well, but the elements should be mostly self-explanatory for users already familiar with the models.

Finally, all of these routines except things that can be coerced to Spark DataFrames -- this usually implies a `tbl_spark` (for dplyr pipelines), but we could definitely implement something that transparently converts a raw data.frame to a Spark DataFrame, and fit models with that. The intention is definitely that users will use 'tbl_spark's when working with sparklyr, though.

---

The other thing we're implementing is Spark's 'feature transformers'; currently a subset are implemented and exported with the 'ft_' prefix. These routines generally take one or more input columns, and generate a new output column formed as a transformation of those columns. A simple example is the binarizer -- it just binarizes a numeric column on some threshold. It could be used like e.g.

```{r}
diamonds_tbl %>%
  sdf_mutate(binary = ft_binarizer(carat, 0.25))
```

  This would generate a new column 'binary' in the dataset, made by thresholding the 'carat' variable.

Note that these are used with a separate interface, sdf_mutate, as their results are computed eagerly, rather than lazily, as with the SQL queries generated by a regular 'mutate'. This implies that in something like:

```{r}

tbl <- diamonds_tbl %>%
  mutate(carat_squared = carat * carat) %>%
  sdf_mutate(binary = ft_binarizer(carat_squared, 0.25))
```


The pending SQL is evaluated, and so the table returned is now a mutated form of the original (rather than just the original table with some pending operations waiting for a future collect). While this computation occurs eagerly, note that we don't 'collect' the dataset; it's still just a Spark DataFrame living over in Spark, but now it's just now a separate entity from the original Spark DataFrame. (Hopefully this all makes sense!)

The other routine that should be interesting here is `sdf_partition` -- it performs a random split (e.g. for testing + training), and returns a list of tbl_sparks where you could e.g. train a model on one set, and test its predictive power on another. (I'll update the docs with examples so it becomes more obvious how it can be used)

## Examples

### Initialization

```{r}
library(sparklyr)
library(ggplot2)

sc <- spark_connect("local", version = "1.6.1")

iris_tbl <- copy_to(sc, iris, "iris", overwrite = TRUE)
```


### KMeans 

Basing kmeans over Spark on [spark.mllib K-means](http://spark.apache.org/docs/latest/mllib-clustering.html#k-means)

Note that the names of variables within the iris `tbl` have been transformed
(replacing `.` with `_`) to work around an issue in the Spark 2.0.0-preview
used in constructing this document -- we expect the issue to be resolved with
the release of Spark 2.0.0.

```{r}
model <- iris_tbl %>%
  select(Petal_Width, Petal_Length) %>%
  ml_kmeans(centers = 3)

iris_tbl %>%
  select(Petal_Width, Petal_Length) %>%
  collect %>%
  ggplot(aes(Petal_Length, Petal_Width)) +
    geom_point(data = model$centers, aes(Petal_Width, Petal_Length), size = 60, alpha = 0.1) +
    geom_point(aes(Petal_Width, Petal_Length), size = 2, alpha = 0.5)

```


### Linear Regression

```{r}
model <- iris_tbl %>%
  select(Petal_Width, Petal_Length) %>%
  ml_linear_regression(response = "Petal_Length", features = c("Petal_Width"))

iris_tbl %>%
  select(Petal_Width, Petal_Length) %>%
  collect %>%
  ggplot(aes(Petal_Length, Petal_Width)) +
    geom_point(aes(Petal_Width, Petal_Length), size = 2, alpha = 0.5) +
    geom_abline(aes(slope = coef(model)[["Petal_Width"]],
                    intercept = coef(model)[["(Intercept)"]],
                    color = "red"))
```

### Logistic Regression

```{r}
# Prepare beaver dataset
beaver <- beaver2
beaver$activ <- factor(beaver$activ, labels = c("Non-Active", "Active"))
copy_to(sc, beaver, "beaver")
beaver_tbl <- tbl(sc, "beaver")

model <- beaver_tbl %>%
  mutate(response = as.numeric(activ == "Active")) %>%
  ml_logistic_regression(response = "response", features = "temp")

print(model)
```


### Partitioning 

```{r}
partitions <- tbl(sc, "iris") %>%
  sdf_partition(training = 0.75, test = 0.25, seed = 1099)

fit <- partitions$training %>%
  ml_linear_regression(response = "Petal_Length", features = c("Petal_Width"))

predict(fit, partitions$test)
```


### Principal Components Analysis 

```{r}
model <- tbl(sc, "iris") %>%
  select(-Species) %>%
  ml_pca()
print(model)
```

### Random Forests

```{r}
mForest <- iris_tbl %>%
  ml_random_forest(
    response = "Species",
    features = c("Petal_Length", "Petal_Width"),
    max.bins = 32L,
    max.depth = 5L,
    num.trees = 20L
  )
mPredict <- predict(mForest, iris_tbl)
head(mPredict)
```


